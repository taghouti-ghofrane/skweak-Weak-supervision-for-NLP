# Comparison of automatic annotations and the mix between automatic and manual annotations provided by UBIAI

<br>
<p align="center">
   <img alt="skweak logo" src="https://github.com/taghouti-ghofrane/skweak-Weak-supervision-for-NLP/blob/main/data/logo.png"/>
</p><br>

# Automatic annotations
In the last few years, the real impact of machine learning (ML) has increased significantly.
in large part, this is due to the advent of deep learning models, which allow practitioners to get state-of-the-art scores on benchmark datasets without any hand-engineered features.
There is a hidden catch, however: the reliance of these models on massive sets of hand-labeled training data.
Manually labeled training sets are costly and time consuming to create .
for these reasons , practitioners have increasingly been turning to weaker forms of supervision, such as heuristically generating training data with external knowledge bases, patterns/rules, or other classifiers. Essentially, these are all ways of programmatically generating training dataâ€”or, more succinctly, programming training data.

## What's Weak Supervision !

Weak supervision is an emerging machine learning paradigm based on a simple idea: instead of labeling data points by hand, we use labeling functions derived from domain knowledge to automatically obtain annotations for a given dataset.

